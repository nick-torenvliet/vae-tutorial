{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca57a1-e8c5-4a78-9bf0-038e65f10cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from scipy.stats import norm\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import DataLoader, Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid as make_image_grid\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "torch.manual_seed(2017)  # reproducibility\n",
    "sns.set_style(\"dark\")\n",
    "\n",
    "# Select device (MPS for Apple Silicon, CUDA for NVIDIA, else CPU)\n",
    "device = torch.device(\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "\n",
    "# Model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=20, hidden_dim=500, input_dim=512):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc_e = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_d1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc_d2 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def encoder(self, x_in):\n",
    "        x = F.relu(self.fc_e(x_in.view(-1, self.input_dim)))\n",
    "        mean = self.fc_mean(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def decoder(self, z):\n",
    "        z = F.relu(self.fc_d1(z))\n",
    "        x_out = torch.sigmoid(self.fc_d2(z))\n",
    "        # Return as (batch, input_dim) so that it matches x_in\n",
    "        return x_out\n",
    "\n",
    "    def sample_normal(self, mean, logvar):\n",
    "        sd = torch.exp(logvar * 0.5)\n",
    "        e = torch.randn(sd.size(), device=device)\n",
    "        z = e.mul(sd).add_(mean)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        z_mean, z_logvar = self.encoder(x_in)\n",
    "        z = self.sample_normal(z_mean, z_logvar)\n",
    "        x_out = self.decoder(z)\n",
    "        return x_out, z_mean, z_logvar\n",
    "\n",
    "\n",
    "# Loss function\n",
    "def criterion(x_out, x_in, z_mu, z_logvar):\n",
    "    # Use reduction='sum' to sum over all elements\n",
    "    bce_loss = F.binary_cross_entropy(x_out, x_in, reduction=\"sum\")\n",
    "    kld_loss = -0.5 * torch.sum(1 + z_logvar - (z_mu**2) - torch.exp(z_logvar))\n",
    "    loss = (bce_loss + kld_loss) / x_out.size(0)  # normalize by batch size\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Generate a Gaussian Dataset\n",
    "class GaussianDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_size=100,\n",
    "        sequence_length=512,\n",
    "        mean=128,\n",
    "        std=64,\n",
    "        drift=64,\n",
    "        nominal_rate=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with random sequences using a Markov chain model.\n",
    "        Each sequence is a vector of length sequence_length.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.dataset_size = dataset_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.drift = drift\n",
    "        self.nominal_rate = nominal_rate\n",
    "\n",
    "        # Generate sequences for both modes\n",
    "        nominal_sequences = int(self.dataset_size * self.nominal_rate)\n",
    "        anominal_sequences = self.dataset_size - nominal_sequences\n",
    "\n",
    "        # Generate mode 1 sequences (first mode)\n",
    "        for _ in range(nominal_sequences):\n",
    "            drift = self.mean + np.random.randint(0, 2 * self.drift + 1) - self.drift\n",
    "            sequence = norm.pdf(\n",
    "                np.arange(self.sequence_length), loc=self.mean + drift, scale=self.std\n",
    "            )\n",
    "            sequence /= sequence.sum()\n",
    "            self.data.append(sequence)\n",
    "            self.labels.append(0)  # Label 0 for mode 1\n",
    "\n",
    "        # Generate mode 2 sequences (second mode)\n",
    "        for _ in range(anominal_sequences):\n",
    "            drift = self.mean + np.random.randint(0, 2 * self.drift + 1) - self.drift\n",
    "            s1 = norm.pdf(\n",
    "                np.arange(self.sequence_length), loc=self.mean + drift, scale=self.std\n",
    "            )\n",
    "            s2 = norm.pdf(\n",
    "                np.arange(self.sequence_length),\n",
    "                loc=np.random.randint(0, self.sequence_length),\n",
    "                scale=np.random.randint(0, drift),\n",
    "            )\n",
    "            sequence = (s1 + s2) / (s1 + s2).sum()\n",
    "            self.data.append(sequence)\n",
    "            self.labels.append(1)  # Label 1 for mode 2\n",
    "\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return sequence and label as tensors.\n",
    "        # Here, the sequence is a vector of length sequence_length.\n",
    "        sequence = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(sequence).float(), torch.tensor(label).float()\n",
    "\n",
    "\n",
    "# instantiate model and optimizer\n",
    "model = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_dataset = GaussianDataset(\n",
    "    dataset_size=10000, sequence_length=512, mean=128, std=16, drift=64, nominal_rate=1\n",
    ")\n",
    "test_dataset = GaussianDataset(\n",
    "    dataset_size=1000,\n",
    "    sequence_length=512,\n",
    "    mean=128,\n",
    "    std=16,\n",
    "    drift=64,\n",
    "    nominal_rate=0.9,\n",
    ")\n",
    "trainloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f88431c-f813-40b6-9042-1e9f75be372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, optimizer, dataloader, epochs=15):\n",
    "    losses = []\n",
    "    for epoch in trange(epochs, desc=\"Epochs\"):\n",
    "        for images, _ in dataloader:\n",
    "            # Ensure images are float and on the proper device\n",
    "            x_in = images.to(device)  # shape: (batch, 512)\n",
    "            optimizer.zero_grad()\n",
    "            x_out, z_mu, z_logvar = model(x_in)\n",
    "            loss = criterion(x_out, x_in, z_mu, z_logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "    return losses\n",
    "\n",
    "\n",
    "train_losses = train(model, optimizer, trainloader)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses)\n",
    "plt.show()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdbd4ef-d151-4a12-ae86-e5b71f6c00ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function (make sure to convert to float)\n",
    "def test(model, dataloader):\n",
    "    loss = []\n",
    "    for images, _ in dataloader:\n",
    "        x_in = images.to(device).float()\n",
    "        x_out, z_mu, z_logvar = model(x_in)\n",
    "        loss.append(criterion(x_out, x_in, z_mu, z_logvar).cpu().detach().numpy())\n",
    "    return loss\n",
    "\n",
    "\n",
    "test_loss = test(model, testloader)\n",
    "plt.plot(test_loss)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3634e09d-9ac9-4350-9f4e-05f43be42c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test and visualize reconstruction using a 2D latent space\n",
    "model2 = VAE(latent_dim=2).to(device)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters())\n",
    "train2_losses = train(model2,optimizer2,trainloader)\n",
    "test2_loss = test(model2,testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621fe2c8-c532-4f0a-8bad-9f4519dcc4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test data encodings on the latent space\n",
    "def visualize_encoder(model,dataloader):\n",
    "    z_means_x, z_means_y, all_labels = [], [], []\n",
    "    \n",
    "    for images,labels in iter(dataloader):\n",
    "        z_means,_ = model.encoder(Variable(images).to(device))\n",
    "        z_means_x = np.append(z_means_x,z_means[:,0].cpu().data.numpy())\n",
    "        z_means_y = np.append(z_means_y,z_means[:,1].cpu().data.numpy())\n",
    "        all_labels = np.append(all_labels,labels.numpy())\n",
    "        \n",
    "    plt.figure(figsize=(6.5,5))\n",
    "    plt.scatter(z_means_x,z_means_y,c=all_labels,cmap='inferno', s=1)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "visualize_encoder(model2,testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac77407-afe4-4c09-b8f1-c5d0215fcf4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
