{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f98448-ba50-4edc-b7c6-163b603090d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from scipy.stats import norm\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import DataLoader, Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid as make_image_grid\n",
    "from tqdm import trange\n",
    "\n",
    "%matplotlib qt\n",
    "torch.manual_seed(2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ad7432-bc07-4f16-86a6-fa8b15bd0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device (MPS for Apple Silicon, CUDA for NVIDIA, else CPU)\n",
    "device = torch.device(\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473a748d-2af1-42a2-9eee-8a0b22b73b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=20, hidden_dim=500, input_dim=512):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc_e = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_d1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc_d2 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "    def encoder(self, x_in):\n",
    "        x = F.relu(self.fc_e(x_in.view(-1, self.input_dim)))\n",
    "        mean = self.fc_mean(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def decoder(self, z):\n",
    "        z = F.relu(self.fc_d1(z))\n",
    "        x_out = torch.sigmoid(self.fc_d2(z))\n",
    "        # Return as (batch, input_dim) so that it matches x_in\n",
    "        return x_out\n",
    "\n",
    "    def sample_normal(self, mean, logvar):\n",
    "        sd = torch.exp(logvar * 0.5)\n",
    "        e = torch.randn(sd.size(), device=device)\n",
    "        z = e.mul(sd).add_(mean)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        z_mean, z_logvar = self.encoder(x_in)\n",
    "        z = self.sample_normal(z_mean, z_logvar)\n",
    "        x_out = self.decoder(z)\n",
    "        return x_out, z_mean, z_logvar\n",
    "\n",
    "# Loss function\n",
    "def criterion(x_out, x_in, z_mu, z_logvar):\n",
    "    # Use reduction='sum' to sum over all elements\n",
    "    bce_loss = F.binary_cross_entropy(x_out, x_in, reduction=\"sum\")\n",
    "    kld_loss = -0.5 * torch.sum(1 + z_logvar - (z_mu**2) - torch.exp(z_logvar))\n",
    "    loss = (bce_loss + kld_loss) / x_out.size(0)  # normalize by batch size\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33244e6-e19d-458a-920e-265aa5365c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate naive Markov chain random sequence dataset\n",
    "class MarkovChainRandomSequenceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_size=100,\n",
    "        sequence_length=512,\n",
    "        full_vocab_size=100,\n",
    "        nominal_vocab_size=90,\n",
    "        nominal_rate=0.99,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with random sequences using a Markov chain model.\n",
    "        Each sequence is a vector of length sequence_length.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.dataset_size = dataset_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.full_vocab_size = full_vocab_size\n",
    "        self.nominal_vocab_size = nominal_vocab_size\n",
    "        self.nominal_rate = nominal_rate\n",
    "\n",
    "        # Generate sequences for both modes\n",
    "        nominal_sequences = int(self.dataset_size * self.nominal_rate)\n",
    "        anominal_sequences = self.dataset_size - nominal_sequences\n",
    "\n",
    "        # Transition matrices\n",
    "        self.P_nominal = self.create_nominal_transition_matrix()\n",
    "        self.P_anominal = self.create_anominal_transition_matrix()\n",
    "\n",
    "        # Generate mode 1 sequences (first mode)\n",
    "        for _ in range(nominal_sequences):\n",
    "            sequence = self._generate_markov_chain_sequence(mode=\"nominal\")\n",
    "            self.data.append(sequence)\n",
    "            self.labels.append(0)  # Label 0 for mode 1\n",
    "\n",
    "        # Generate mode 2 sequences (second mode)\n",
    "        for _ in range(anominal_sequences):\n",
    "            sequence = self._generate_markov_chain_sequence(mode=\"anominal\")\n",
    "            self.data.append(sequence)\n",
    "            self.labels.append(1)  # Label 1 for mode 2\n",
    "\n",
    "        pass\n",
    "\n",
    "    def create_nominal_transition_matrix(self):\n",
    "        P = np.zeros(self.full_vocab_size, dtype=float)\n",
    "        P[: self.nominal_vocab_size] = 1 / self.nominal_vocab_size\n",
    "        return P\n",
    "\n",
    "    def create_anominal_transition_matrix(self):\n",
    "        P = np.ones(self.full_vocab_size, dtype=float) / self.full_vocab_size\n",
    "        return P\n",
    "\n",
    "    def _generate_markov_chain_sequence(self, mode):\n",
    "        # Select the transition matrix based on the mode\n",
    "        transition_matrix = self.P_nominal if mode == \"nominal\" else self.P_anominal\n",
    "\n",
    "        # Start with a random token\n",
    "        if mode == \"nominal\":\n",
    "            current_state = np.random.choice(self.nominal_vocab_size)\n",
    "        else:\n",
    "            current_state = np.random.choice(self.full_vocab_size)\n",
    "\n",
    "        normalized_data = current_state / self.full_vocab_size\n",
    "        sequence = [normalized_data]\n",
    "\n",
    "        # Generate the rest of the sequence using the transition probabilities\n",
    "        for _ in range(self.sequence_length - 1):\n",
    "            current_state = np.random.choice(self.full_vocab_size, p=transition_matrix)\n",
    "            normalized_data = current_state / self.full_vocab_size\n",
    "            sequence.append(normalized_data)\n",
    "        return sequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return sequence and label as tensors.\n",
    "        # Here, the sequence is a vector of length sequence_length.\n",
    "        sequence = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(sequence).float(), torch.tensor(label).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383925cb-2f30-46c9-bc37-a7e5ac8ef3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model and optimizer\n",
    "model = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a748e361-4aad-4e87-8058-4f5038f147dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloaders\n",
    "train_dataset = MarkovChainRandomSequenceDataset(\n",
    "    dataset_size=1000,\n",
    "    sequence_length=512,\n",
    "    full_vocab_size=100,\n",
    "    nominal_vocab_size=90,\n",
    "    nominal_rate=1,\n",
    ")\n",
    "test_dataset = MarkovChainRandomSequenceDataset(\n",
    "    dataset_size=1000,\n",
    "    sequence_length=512,\n",
    "    full_vocab_size=100,\n",
    "    nominal_vocab_size=90,\n",
    "    nominal_rate=0.9,\n",
    ")\n",
    "trainloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e85643a-03be-4380-a5d9-7b9144e46cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, optimizer, dataloader, epochs=15):\n",
    "    losses = []\n",
    "    for epoch in trange(epochs, desc=\"Epochs\"):\n",
    "        for images, _ in dataloader:\n",
    "            # Ensure images are float and on the proper device\n",
    "            x_in = images.to(device)  # shape: (batch, 512)\n",
    "            optimizer.zero_grad()\n",
    "            x_out, z_mu, z_logvar = model(x_in)\n",
    "            loss = criterion(x_out, x_in, z_mu, z_logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "    return losses\n",
    "train_losses = train(model, optimizer, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdb913a-a138-4879-92c3-1393c80f6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2026317-1c54-45c0-ae7f-a871cfaae0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function (make sure to convert to float)\n",
    "def test(model, dataloader):\n",
    "    loss = []\n",
    "    for images, _ in dataloader:\n",
    "        x_in = images.to(device).float()\n",
    "        x_out, z_mu, z_logvar = model(x_in)\n",
    "        loss.append(criterion(x_out, x_in, z_mu, z_logvar).cpu().detach().numpy())\n",
    "    return loss\n",
    "\n",
    "test_loss = test(model, testloader)\n",
    "plt.plot(test_loss)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b44058-a2e3-4b78-b870-d136cd0470a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test and visualize reconstruction using a 2D latent space\n",
    "model2 = VAE(latent_dim=2).to(device)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters())\n",
    "train2_losses = train(model2,optimizer2,trainloader)\n",
    "test2_loss = test(model2,testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7798f8-716b-4381-8528-c7a91b8e89bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test data encodings on the latent space\n",
    "def visualize_encoder(model,dataloader):\n",
    "    z_means_x, z_means_y, all_labels = [], [], []\n",
    "    \n",
    "    for images,labels in iter(dataloader):\n",
    "        z_means,_ = model.encoder(Variable(images).to(device))\n",
    "        z_means_x = np.append(z_means_x,z_means[:,0].cpu().data.numpy())\n",
    "        z_means_y = np.append(z_means_y,z_means[:,1].cpu().data.numpy())\n",
    "        all_labels = np.append(all_labels,labels.numpy())\n",
    "        \n",
    "    plt.figure(figsize=(6.5,5))\n",
    "    plt.scatter(z_means_x,z_means_y,c=all_labels,cmap='viridis', s=1)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "visualize_encoder(model2,testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56560845-5534-4152-8e32-89103d6780bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
